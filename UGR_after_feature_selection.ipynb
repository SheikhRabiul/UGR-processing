{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"intro\"></a>\n",
    "## UGR Dataset\n",
    "\n",
    "11 features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Starting Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"rule-based-models\"></a>\n",
    "## Case1 Data scientist: Boolean Rule and Logistic Rule Regression models\n",
    "In evaluating a machine learning model for deployment, a data scientist would ideally like to understand the behavior of the model as a whole, not just in specific instances (e.g. specific loan applicants). This is especially true in regulated industries such as banking where higher standards of explainability may be required. For example, the data scientist may have to present the model to: 1) technical and business managers for review before deployment, 2) a lending expert to compare the model to the expert's knowledge, or 3) a regulator to check for compliance. Furthermore, it is common for a model to be deployed in a different geography than the one it was trained on. A global view of the model may uncover problems with overfitting and poor generalization to other geographies before deployment.\n",
    "\n",
    "Directly interpretable models can provide such global understanding because they have a sufficiently simple form for their workings to be transparent. Below we present two directly interpretable models in the form of a [Boolean rule (BR)](#BRCG) and a [logistic rule regression (LogRR)](#LogRR) model. The former is produced by the Boolean Rule Column Generation (BRCG) algorithm while the latter is a generalized linear rule model (GLRM), both implemented in AIX360. While both models are interpretable, they provide different trade-offs between model simplicity and accuracy in predicting loan repayment. BRCG yields a very simple set of rules that has reasonable accuracy. LogRR achieves higher accuracy, higher even than some uninterpretable models, while retaining the form of a linear model. Its interpretation is enhanced by [plots as demonstrated below](#visualize)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C1. Load and process data for BRCG and LogRR\n",
    "The setting `custom_preprocessing=nan_preprocessing` converts special values in the data (coded as negative integers) to `np.nan`, which can be handled properly by BRCG and LogRR, as opposed to replacing them with zeros or mean values. The data is then split into training and test sets using a fixed random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using UGR dataset:  /home/ec2-user/aix360/AIX-UGR/aix360/datasets/../data/heloc_data/ugr_dataset.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>707182</th>\n",
       "      <th>2292866</th>\n",
       "      <th>497947</th>\n",
       "      <th>1931563</th>\n",
       "      <th>2388218</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Timestamp</th>\n",
       "      <td>6.480000e+04</td>\n",
       "      <td>6.480000e+04</td>\n",
       "      <td>6.480000e+04</td>\n",
       "      <td>6.480000e+04</td>\n",
       "      <td>6.480000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FlowDuration</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SrcIP</th>\n",
       "      <td>2.748889e+09</td>\n",
       "      <td>3.428946e+09</td>\n",
       "      <td>3.746848e+09</td>\n",
       "      <td>3.156165e+09</td>\n",
       "      <td>3.330555e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DstIP</th>\n",
       "      <td>7.190364e+08</td>\n",
       "      <td>7.190346e+08</td>\n",
       "      <td>7.190352e+08</td>\n",
       "      <td>7.190350e+08</td>\n",
       "      <td>7.190328e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SrcPort</th>\n",
       "      <td>5.118100e+04</td>\n",
       "      <td>6.514000e+04</td>\n",
       "      <td>1.732000e+03</td>\n",
       "      <td>4.865300e+04</td>\n",
       "      <td>2.688900e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DstPort</th>\n",
       "      <td>5.300000e+01</td>\n",
       "      <td>5.300000e+01</td>\n",
       "      <td>8.000000e+01</td>\n",
       "      <td>8.000000e+01</td>\n",
       "      <td>3.784000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Protocol</th>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TypeofService</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PacketExed</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    707182        2292866       497947        1931563  \\\n",
       "Timestamp      6.480000e+04  6.480000e+04  6.480000e+04  6.480000e+04   \n",
       "FlowDuration   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "SrcIP          2.748889e+09  3.428946e+09  3.746848e+09  3.156165e+09   \n",
       "DstIP          7.190364e+08  7.190346e+08  7.190352e+08  7.190350e+08   \n",
       "SrcPort        5.118100e+04  6.514000e+04  1.732000e+03  4.865300e+04   \n",
       "DstPort        5.300000e+01  5.300000e+01  8.000000e+01  8.000000e+01   \n",
       "Protocol       2.000000e+00  2.000000e+00  1.000000e+00  1.000000e+00   \n",
       "TypeofService  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "PacketExed     0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "\n",
       "                    2388218  \n",
       "Timestamp      6.480000e+04  \n",
       "FlowDuration   0.000000e+00  \n",
       "SrcIP          3.330555e+09  \n",
       "DstIP          7.190328e+08  \n",
       "SrcPort        2.688900e+04  \n",
       "DstPort        3.784000e+03  \n",
       "Protocol       1.000000e+00  \n",
       "TypeofService  0.000000e+00  \n",
       "PacketExed     0.000000e+00  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas\n",
    "\n",
    "# Load FICO HELOC data with special values converted to np.nan\n",
    "from aix360.datasets.ugr_dataset import UGRDataset, nan_preprocessing\n",
    "data = UGRDataset(custom_preprocessing=nan_preprocessing).data()\n",
    "# Separate target variable\n",
    "y = data.pop('Label')\n",
    "\n",
    "# Split data into training and test sets using fixed random seed\n",
    "from sklearn.model_selection import train_test_split\n",
    "dfTrain, dfTest, yTrain, yTest = train_test_split(data, y, random_state=0, stratify=y)\n",
    "dfTrain.head().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BRCG and LogRR require non-binary features to be binarized using the provided `FeatureBinarizer` class. We use the default of nine quantile thresholds (i.e. 10 bins) to binarize ordinal (including continuous-valued) features, include all negations (e.g. '>' comparisons as well as '<='), and also return standardized versions of the original unbinarized ordinal features, which are used by LogRR but not BRCG. Below is the result of binarizing the first 'ExternalRiskEstimate' feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<aix360.algorithms.rbm.features.FeatureBinarizer object at 0x7f64f0722710>\n",
      "operation           <=                                                      \\\n",
      "value     9.527502e+08 1.201087e+09 2.165614e+09 2.403863e+09 2.821066e+09   \n",
      "707182               0            0            0            0            1   \n",
      "2292866              0            0            0            0            0   \n",
      "497947               0            0            0            0            0   \n",
      "1931563              0            0            0            0            0   \n",
      "2388218              0            0            0            0            0   \n",
      "\n",
      "operation                                                                >  \\\n",
      "value     3.267393e+09 3.494333e+09 3.550906e+09 3.687106e+09 9.527502e+08   \n",
      "707182               1            1            1            1            1   \n",
      "2292866              0            1            1            1            1   \n",
      "497947               0            0            0            0            1   \n",
      "1931563              1            1            1            1            1   \n",
      "2388218              0            1            1            1            1   \n",
      "\n",
      "operation                                                                   \\\n",
      "value     1.201087e+09 2.165614e+09 2.403863e+09 2.821066e+09 3.267393e+09   \n",
      "707182               1            1            1            0            0   \n",
      "2292866              1            1            1            1            1   \n",
      "497947               1            1            1            1            1   \n",
      "1931563              1            1            1            1            0   \n",
      "2388218              1            1            1            1            1   \n",
      "\n",
      "operation                                         \n",
      "value     3.494333e+09 3.550906e+09 3.687106e+09  \n",
      "707182               0            0            0  \n",
      "2292866              0            0            0  \n",
      "497947               1            1            1  \n",
      "1931563              0            0            0  \n",
      "2388218              0            0            0  \n"
     ]
    }
   ],
   "source": [
    "# Binarize data and also return standardized ordinal features\n",
    "from aix360.algorithms.rbm import FeatureBinarizer\n",
    "fb = FeatureBinarizer(negations=True, returnOrd=True)\n",
    "print(fb)\n",
    "dfTrain, dfTrainStd = fb.fit_transform(dfTrain)\n",
    "dfTest, dfTestStd = fb.transform(dfTest)\n",
    "print(dfTrain['SrcIP'].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"BRCG\"></a>\n",
    "### C2. Run Boolean Rule Column Generation (BRCG)\n",
    "First we consider BRCG, which is designed to produce a very simple OR-of-ANDs rule (known more formally as disjunctive normal form, DNF) or alternatively an AND-of-ORs rule (conjunctive normal form, CNF) to predict whether an applicant will repay the loan on time (Y = 1). For a binary classification problem such as we have here, a DNF rule is equivalent to a *rule set*, where AND clauses in the DNF correspond to individual rules in the rule set. Furthermore, it can be shown that a CNF rule for Y = 1 is equivalent to a DNF rule for Y = 0 [[1]](https://ieeexplore.ieee.org/document/7738856). BRCG is distinguished by its use of the optimization technique of column generation to search the space of possible clauses, which is exponential in size. To learn more about column generation, please see our NeurIPS paper [[2]](http://papers.nips.cc/paper/7716-boolean-decision-rules-via-column-generation). \n",
    "\n",
    "For this dataset, we find that a CNF rule for Y = 1 (i.e. a DNF for Y = 0, enabled by setting `CNF=True`) is slightly better than a DNF rule for Y = 1. The model complexity parameters `lambda0` and `lambda1` penalize the number of clauses in the rule and the number of conditions in each clause. We use the default values of 1e-3 for `lambda0` and `lambda1` (decreasing them did not increase accuracy here) and leave other parameters at their defaults as well. The model is then trained, evaluated, and printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning CNF rule with complexity parameters lambda0=0.001, lambda1=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/miniconda3/envs/aix360/lib/python3.6/site-packages/cvxpy/expressions/expression.py:516: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n",
      "/home/ec2-user/miniconda3/envs/aix360/lib/python3.6/site-packages/cvxpy/expressions/expression.py:516: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n",
      "/home/ec2-user/miniconda3/envs/aix360/lib/python3.6/site-packages/cvxpy/expressions/expression.py:516: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "\n",
      "  warnings.warn(__STAR_MATMUL_WARNING__, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial LP solved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/aix360/AIX-UGR/aix360/algorithms/rbm/beam_search.py:58: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  zOut = pd.Series(index=X.columns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9904547738989636\n",
      "Test accuracy: 0.9904553250381353\n",
      "Predict Y=0 if ANY of the following rules are satisfied, otherwise Y=1:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate BRCG with small complexity penalty and large beam search width\n",
    "from aix360.algorithms.rbm import BooleanRuleCG\n",
    "br = BooleanRuleCG(lambda0=1e-3, lambda1=1e-3, CNF=True)\n",
    "\n",
    "# Train, print, and evaluate model\n",
    "br.fit(dfTrain, yTrain)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Training accuracy:', accuracy_score(yTrain, br.predict(dfTrain)))\n",
    "print('Test accuracy:', accuracy_score(yTest, br.predict(dfTest)))\n",
    "print('Predict Y=0 if ANY of the following rules are satisfied, otherwise Y=1:')\n",
    "print(br.explain()['rules'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned DNF rule for Y = 0 is indeed very simple with only two clauses, each involving the same two features. It is interesting to see that such a rule can already achieve 69.7% accuracy. 'ExternalRiskEstimate' is a consolidated version of some risk markers (higher is better), while 'NumSatisfactoryTrades' is the number of satisfactory credit accounts. It makes sense therefore that for applicants with more than 17 satisfactory accounts, the ExternalRiskEstimate threshold dividing good (Y = 1) and bad (Y = 0) credit risk is slightly lower (more lenient) than for applicants with fewer satisfactory accounts.\n",
    "\n",
    "We note that AIX360 includes only a heuristic beam search version of BRCG. The published version of BRCG [[2]](http://papers.nips.cc/paper/7716-boolean-decision-rules-via-column-generation) (not implemented in AIX360) uses integer programming to yield slightly more complex rules that are also more accurate (close to 72% test accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"LogRR\"></a>\n",
    "### C3. Run Logistic Rule Regression (LogRR)\n",
    "Next we consider a LogRR model, which can improve accuracy at the cost of a more complex but still interpretable model. Specifically, LogRR fits a logistic regression model using rule-based features, where column generation is again used to generate promising candidates from the space of all possible rules. Here we are also including unbinarized ordinal features (`useOrd=True`) in addition to rules. Similar to BRCG, the complexity parameters `lambda0`, `lambda1` penalize the number of rules included in the model and the number of conditions in each rule. the The values for `lambda0`, `lambda1` below strike a good balance between accuracy and model complexity, based on our published experience with the FICO HELOC dataset [[3]](http://proceedings.mlr.press/v97/wei19a.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/miniconda3/envs/aix360/lib/python3.6/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/ec2-user/aix360/AIX-UGR/aix360/algorithms/rbm/beam_search.py:58: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  zOut = pd.Series(index=X.columns)\n",
      "/home/ec2-user/miniconda3/envs/aix360/lib/python3.6/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/ec2-user/aix360/AIX-UGR/aix360/algorithms/rbm/beam_search.py:58: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  zOut = pd.Series(index=X.columns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9904547738989636\n",
      "Test accuracy: 0.9904553250381353\n",
      "Probability of Y=1 is predicted as logistic(z) = 1 / (1 + exp(-z))\n",
      "where z is a linear combination of the following rules/numerical features:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rule</th>\n",
       "      <th>coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(intercept)</td>\n",
       "      <td>7.51492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DstIP &lt;= 719036095.00 AND DstIP &gt; 719035603.00</td>\n",
       "      <td>-7.24444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DstIP &lt;= 719035992.00</td>\n",
       "      <td>2.70018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DstIP &lt;= 719035603.00</td>\n",
       "      <td>-0.911487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             rule coefficient\n",
       "0                                     (intercept)     7.51492\n",
       "1  DstIP <= 719036095.00 AND DstIP > 719035603.00    -7.24444\n",
       "2                           DstIP <= 719035992.00     2.70018\n",
       "3                           DstIP <= 719035603.00   -0.911487"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate LRR with good complexity penalties and numerical features\n",
    "from aix360.algorithms.rbm import LogisticRuleRegression\n",
    "lrr = LogisticRuleRegression(lambda0=0.005, lambda1=0.001, useOrd=True)\n",
    "\n",
    "# Train, print, and evaluate model\n",
    "lrr.fit(dfTrain, yTrain, dfTrainStd)\n",
    "print('Training accuracy:', accuracy_score(yTrain, lrr.predict(dfTrain, dfTrainStd)))\n",
    "print('Test accuracy:', accuracy_score(yTest, lrr.predict(dfTest, dfTestStd)))\n",
    "print('Probability of Y=1 is predicted as logistic(z) = 1 / (1 + exp(-z))')\n",
    "print('where z is a linear combination of the following rules/numerical features:')\n",
    "lrr.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test accuracy of LogRR is significantly better than that of BRCG and even better than the neural network in the [Loan Officer](#c2) and [Customer](#contrastive) sections. The LogRR model remains directly interpretable as it is a logistic regression model that uses the 36 rule-based and ordinal features shown above (in addition to an intercept term). Rules are distinguished by having one or more conditions on feature values (e.g. AverageMInFile <= 52.0) while ordinal features are marked by just the feature name without conditions (e.g. ExternalRiskEstimate). Being a linear model, feature importance is naturally given by the model coefficients and thus the list is sorted in order of decreasing coefficient magnitude. The list can be truncated if the user wishes to display fewer features.\n",
    "\n",
    "Since the rules in this LogRR model happen to all be single conditions on individual features, the model contains no interactions between features. It is therefore a kind of [generalized additive model (GAM)](https://en.wikipedia.org/wiki/Generalized_additive_model), i.e. a sum of functions of individual features, where these functions are themselves sums of step function components from rules and linear components from unbinarized ordinal features. Thus a better way to visualize the model is by plotting the univariate functions that make up the GAM, as we do next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"visualize\"></a>\n",
    "### C4. Visualize LogRR model as a Generalized Additive Model (GAM)\n",
    "We use the `visualize()` method of `LogisticRuleRegression` to plot the functions in the GAM that corresponds to the LogRR model (more generally, `visualize()` plots the GAM part of a LogRR model, excluding higher-degree rules). The plots show the sizes and shapes of the model's dependences on individual features. These can then be compared to a lending expert's knowledge. In the present case, all plots indicate that the model behaves as we would expect with some interesting nuances. \n",
    "\n",
    "The 36 features shown above involve only 14 of the original features in the data (not including the intercept), as verified below. For example, ExternalRiskEstimate appears in its unbinarized form in row 2 above and also in 3 rules (rows 14, 23, 34)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n",
      "LogisticRuleRegression(lambda0=0.005, lambda1=0.001, useOrd=True)\n"
     ]
    }
   ],
   "source": [
    "dfx = lrr.explain()\n",
    "#print(dfx)\n",
    "# Separate 1st-degree rules into (feature, operation, value) to count unique features\n",
    "#dfx2 = dfx['rule/numerical feature'].str.split(' ', expand=True)\n",
    "dfx2 = dfx['rule'].str.split(' ', expand=True)\n",
    "#print(dfx2)\n",
    "dfx2.columns = ['feature1','operation','value', 'connector', 'feature2', 'operation', 'value']\n",
    "print(dfx2['feature1'].nunique())# includes intercept\n",
    "print(dfx2['feature2'].nunique())# includes intercept\n",
    "\n",
    "print(lrr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It follows that there are 14 functions to plot, which we organize into semantic groups below to ease interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ExternalRiskEstimate\n",
    "As expected from the BRCG Boolean rule above, 'ExternalRiskEstimate' is an important feature positively correlated with good credit risk. The jumps in the plot indicate that applicants with above average 'ExternalRiskEstimate' (the mean is 72) get an additional boost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/aix360/AIX-UGR/aix360/algorithms/rbm/logistic_regression.py:351: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  terms = pd.Series(index=pd.MultiIndex.from_arrays([[],[],[]], names=self.z.index.names))\n"
     ]
    }
   ],
   "source": [
    "lrr.visualize(data, fb, ['Timestamp']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Credit inquiries\n",
    "The next two plots illustrate the dependence on the applicant's credit inquiries. The first plot shows a significant penalty for having less than one month since the most recent inquiry ('MSinceMostRecentInqexcl7days' = 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lrr.visualize(data, fb, ['FlowDuration']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second shows that predicted risk increases with the number of inquiries in the last six months ('NumInqLast6M')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrr.visualize(data, fb, ['SrcIP']);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Debt level\n",
    "The following four plots relate to the applicant's debt level. 'NetFractionRevolvingBurden' is the ratio of revolving debt (e.g. credit card) balance to credit limit, expressed as a percentage, and has a large negative impact on the probability of good credit. A small fraction of applicants (less than 1%) actually have NetFractionRevolvingBurden greater than 100%, i.e. more revolving debt than their credit limit. This might be investigated further by the data scientist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrr.visualize(data, fb, ['DstIP']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second 'NumBank2NatlTradesWHighUtilization' plot shows that the number of accounts (\"trades\") with high utilization (high balance relative to credit limit for each account) also has a large impact, with a drop as soon as one account has high utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/aix360/AIX-UGR/aix360/algorithms/rbm/logistic_regression.py:351: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  terms = pd.Series(index=pd.MultiIndex.from_arrays([[],[],[]], names=self.z.index.names))\n"
     ]
    }
   ],
   "source": [
    "lrr.visualize(data, fb, ['SrcPort']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The third plot shows that the model gives a bonus to applicants who carry balances on no more than five revolving debt accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrr.visualize(data, fb, ['DstPort']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fourth shows an effect from the percentage of accounts with a balance that is much smaller than those from other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrr.visualize(data, fb, ['Protocol']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number and type of accounts\n",
    "The number of \"satisfactory\" accounts (\"trades\") has a significant positive effect on the predicted probability of good credit, with jumps at 12 and 17 accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lrr.visualize(data, fb, ['Forwardingstatus']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, having more than 40% as installment debt accounts (e.g. car loans) is seen as a negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrr.visualize(data, fb, ['TypeofService']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Length of credit history\n",
    "The 'AverageMInFile' plot shows that most of the benefit of having a longer average credit history accrues between average ages of 52 and 84 months (four to seven years). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrr.visualize(data, fb, ['PackageExed']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar but smaller gains come when the age of the oldest account ('MSinceOldestTradeOpen') exceeds 122 and 146 months (10-12 years)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lrr.visualize(data, fb, ['Bytes']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delinquencies\n",
    "The last set of plots looks at the effect of delinquencies. The first plot shows that much of the change due to the percentage of accounts that were never delinquent ('PercentTradesNeverDelq') occurs between 90% and 100%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lrr.visualize(data, fb, ['PercentTradesNeverDelq']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'MaxDelq2PublicRecLast12M' measures the severity of the applicant's worst delinquency from the last 12 months of the public record. A value of 5 or below indicates that some delinquency has occurred, whether of unknown duration, 30/60/90/120 days delinquent, or a derogatory comment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lrr.visualize(data, fb, ['MaxDelq2PublicRecLast12M']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the last 'MSinceMostRecentDelq' plot, the effect of the most recent delinquency wears off after 21 months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lrr.visualize(data, fb, ['MSinceMostRecentDelq']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"prototypes\"></a>\n",
    "## Case 3. Loan Officer: Prototypical explanations for HELOC use case\n",
    "\n",
    "We now show how to generate explanations in the form of selecting prototypical or similar user profiles to an applicant in question that a bank employee such as a loan officer may be interested in. This may help the employee understand the decision of an applicant's HELOC application being accepted or rejected in the context of other similar applications. Note that the selected prototypical applications are profiles that are part of the training set that has been used to train an AI model that predicts good or bad i.e. approved or rejected for these applications. In fact, the method used in this notebook can work even if we are given not just one but a set of user profiles for which we want to find similar profiles from a training dataset. Additionally, the method computes weights for each prototype showcasing its similarity to the user(s) in question.\n",
    "\n",
    "The prototypical explanations in AIX360 are obtained using the Protodash algorithm developed in the following work: [ProtoDash: Fast Interpretable Prototype Selection](https://arxiv.org/abs/1707.01212)\n",
    "\n",
    "We now provide a brief overview of the method. The method takes as input a datapoint (or group of datapoints) that we want to explain with respect to instances in a training set belonging to the same feature space. The method then tries to minimize the maximum mean discrepancy (MMD metric) between the datapoints we want to explain and a prespecified number of instances from the training set that it will select. In other words, it will try to select training instances that have the same distribution as the datapoints we want to explain. The method does greedy selection and has quality guarantees with it also returning importance weights for the chosen prototypical training instances indicative of how similar/representative they are.\n",
    "\n",
    "In this tutorial, we will see two examples of obtaining prototypes, one for a user whose HELOC application was approved and another for a user whose HELOC application was rejected. In each case, we showcase the top five prototypes from the training data along with how similar the feature values were for these prototypes.\n",
    "\n",
    "[Example 1. Obtaining similar samples as explanations for a HELOC applicant predicted as \"Good\"](#good)<br>\n",
    "[Example 2. Obtaining similar samples as explanations for a HELOC applicant predicted as \"Bad\"](#bad)<br>\n",
    "\n",
    "\n",
    "###### Why Protodash?\n",
    "Before we showcase the two examples we provide some motivation for using this method. The method selects applications from the training set that are similar in different ways to the user application we want to explain. For example, a users loan may be rejected justifiably because the number of satisfactory trades he performed were low similar to another rejected user, or because his/her debts were too high similar to a different rejected user. Either of these reasons in isolation may be sufficient for rejection and the method is able to surface a variety of such reasons through the selected prototypes. This is not the case using standard nearest neighbor techniques which use metrics such as euclidean distance, cosine similarity amongst others, where one might get the same type of explanation (i.e. applications with only low number of satisfactory trades). Protodash thus is able to provide a much more well rounded and comprehensive view of why the decision for the applicant may be justifiable.\n",
    "\n",
    "Another benefit of the method is that — since it does distribution matching between the user/users in question and those available in the training set — it could, in principle, be applied also in non-iid settings such as for time series data. Other approaches which find similar profiles using standard distance measures (viz. euclidean, cosine) do not have this property. Additionally, we can also highlight important features for the different prototypes that made them similar to the user/users in question.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import statements\n",
    "\n",
    "Import necessary libraries, frameworks and algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model, load_model, model_from_json\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "from aix360.algorithms.contrastive import CEMExplainer, KerasClassifier\n",
    "from aix360.algorithms.protodash import ProtodashExplainer\n",
    "from aix360.datasets.ugr_dataset import UGRDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load UGR dataset and show sample applicants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using UGR dataset:  /home/ec2-user/aix360/AIX-UGR/aix360/datasets/../data/heloc_data/ugr_dataset.csv\n",
      "Size of UGR dataset: (2488505, 10)\n",
      "Number of \"Normal\" connection: 2464752\n",
      "Number of \"Abnormal\" connection: 23753\n",
      "Sample connections:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Timestamp</th>\n",
       "      <td>64800</td>\n",
       "      <td>64800</td>\n",
       "      <td>64800</td>\n",
       "      <td>64800</td>\n",
       "      <td>64800</td>\n",
       "      <td>64800</td>\n",
       "      <td>64800</td>\n",
       "      <td>64800</td>\n",
       "      <td>64800</td>\n",
       "      <td>64800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FlowDuration</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SrcIP</th>\n",
       "      <td>1816330234</td>\n",
       "      <td>1816330234</td>\n",
       "      <td>1816330234</td>\n",
       "      <td>3230619516</td>\n",
       "      <td>3230619516</td>\n",
       "      <td>1816330234</td>\n",
       "      <td>1816330234</td>\n",
       "      <td>1816330234</td>\n",
       "      <td>1816330234</td>\n",
       "      <td>1816330234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DstIP</th>\n",
       "      <td>719035615</td>\n",
       "      <td>719035615</td>\n",
       "      <td>719035615</td>\n",
       "      <td>719035615</td>\n",
       "      <td>719035615</td>\n",
       "      <td>719035615</td>\n",
       "      <td>719035615</td>\n",
       "      <td>719035615</td>\n",
       "      <td>719035615</td>\n",
       "      <td>719035615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SrcPort</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DstPort</th>\n",
       "      <td>56713</td>\n",
       "      <td>49626</td>\n",
       "      <td>41566</td>\n",
       "      <td>55955</td>\n",
       "      <td>40156</td>\n",
       "      <td>50320</td>\n",
       "      <td>47617</td>\n",
       "      <td>35493</td>\n",
       "      <td>38270</td>\n",
       "      <td>35255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Protocol</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TypeofService</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PacketExed</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Label</th>\n",
       "      <td>abnormal</td>\n",
       "      <td>abnormal</td>\n",
       "      <td>abnormal</td>\n",
       "      <td>abnormal</td>\n",
       "      <td>abnormal</td>\n",
       "      <td>abnormal</td>\n",
       "      <td>abnormal</td>\n",
       "      <td>abnormal</td>\n",
       "      <td>abnormal</td>\n",
       "      <td>abnormal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        0           1           2           3           4           5           6           7           8           9\n",
       "Timestamp           64800       64800       64800       64800       64800       64800       64800       64800       64800       64800\n",
       "FlowDuration            0           0           0           0           0           0           0           0           0           0\n",
       "SrcIP          1816330234  1816330234  1816330234  3230619516  3230619516  1816330234  1816330234  1816330234  1816330234  1816330234\n",
       "DstIP           719035615   719035615   719035615   719035615   719035615   719035615   719035615   719035615   719035615   719035615\n",
       "SrcPort                25          25          25          25          25          25          25          25          25          25\n",
       "DstPort             56713       49626       41566       55955       40156       50320       47617       35493       38270       35255\n",
       "Protocol                1           1           1           1           1           1           1           1           1           1\n",
       "TypeofService           0           0           0           0           0           0           0           0           0           0\n",
       "PacketExed              0           0           0           0           0           0           0           0           0           0\n",
       "Label            abnormal    abnormal    abnormal    abnormal    abnormal    abnormal    abnormal    abnormal    abnormal    abnormal"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heloc = UGRDataset()\n",
    "df = heloc.dataframe()\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 24)\n",
    "pd.set_option('display.width', 1000)\n",
    "print(\"Size of UGR dataset:\", df.shape)\n",
    "print(\"Number of \\\"Normal\\\" connection:\", np.sum(df['Label']=='normal'))\n",
    "print(\"Number of \\\"Abnormal\\\" connection:\", np.sum(df['Label']=='abnormal'))\n",
    "print(\"Sample connections:\")\n",
    "df.head(10).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of FlowDuration and Bytes columns:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['Bytes'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-2d7fbd9f51de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Plot (example) distributions for two features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Distribution of FlowDuration and Bytes columns:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FlowDuration'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Bytes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/aix360/lib/python3.6/site-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36mhist_frame\u001b[0;34m(data, column, by, grid, xlabelsize, xrot, ylabelsize, yrot, ax, sharex, sharey, figsize, layout, bins, backend, **kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mlayout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m     )\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/aix360/lib/python3.6/site-packages/pandas/plotting/_matplotlib/hist.py\u001b[0m in \u001b[0;36mhist_frame\u001b[0;34m(data, column, by, grid, xlabelsize, xrot, ylabelsize, yrot, ax, sharex, sharey, figsize, layout, bins, **kwds)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCIndexClass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mcolumn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_numeric_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0mnaxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/aix360/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2804\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2805\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2806\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2808\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/aix360/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         self._validate_read_indexer(\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m         )\n\u001b[1;32m   1555\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/aix360/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1644\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"loc\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1645\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1646\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1648\u001b[0m             \u001b[0;31m# we skip the warning on Categorical/Interval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Bytes'] not in index\""
     ]
    }
   ],
   "source": [
    "# Plot (example) distributions for two features\n",
    "print(\"Distribution of FlowDuration and Bytes columns:\")\n",
    "hist = df.hist(column=['FlowDuration', 'Bytes'], bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"c1\"></a>\n",
    "### Step 1: Process and Normalize  dataset for training\n",
    "\n",
    "We will first process the HELOC dataset before using it to train an NN model that can predict the\n",
    "target variable RiskPerformance. The HELOC dataset is a tabular dataset with numerical values. However, some of the values are negative and need to be filtered. The processed data is stored in the file heloc.npz for easy access. The dataset is also normalized for training.\n",
    "\n",
    "The data processing and the type of model built in this case is different from the Data Scientist persona described above where rule based methods are showcased. This is the reason for going through these steps again for the Loan Officer persona."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Process the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data and split dataset into train/test\n",
    "(Data, x_train, x_test, y_train_b, y_test_b) = heloc.split()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### b. Normalize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.vstack((x_train, x_test))\n",
    "Zmax = np.max(Z, axis=0)\n",
    "Zmin = np.min(Z, axis=0)\n",
    "\n",
    "#normalize an array of samples to range [-0.5, 0.5]\n",
    "def normalize(V):\n",
    "    VN = (V - Zmin)/(Zmax - Zmin)\n",
    "    VN = VN - 0.5\n",
    "    return(VN)\n",
    "    \n",
    "# rescale a sample to recover original values for normalized values. \n",
    "def rescale(X):\n",
    "    return(np.multiply ( X + 0.5, (Zmax - Zmin) ) + Zmin)\n",
    "\n",
    "N = normalize(Z)\n",
    "xn_train = N[0:x_train.shape[0], :]\n",
    "xn_test  = N[x_train.shape[0]:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"c2\"></a>\n",
    "### Step 2. Define and train a NN classifier\n",
    "\n",
    "Let us now build a loan approval model based on the HELOC dataset.\n",
    "\n",
    "#### a. Define NN architecture\n",
    "We now define the architecture of a 2-layer neural network classifier whose predictions we will try to interpret. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn with no softmax\n",
    "def nn_small():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=11, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(2, kernel_initializer='normal'))    \n",
    "    return model    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Train the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for repeatability\n",
    "np.random.seed(1) \n",
    "tf.set_random_seed(2) \n",
    "\n",
    "class_names = ['background', 'blacklist']\n",
    "\n",
    "# loss function\n",
    "def fn(correct, predicted):\n",
    "    return tf.nn.softmax_cross_entropy_with_logits(labels=correct, logits=predicted)\n",
    "\n",
    "# compile and print model summary\n",
    "nn = nn_small()\n",
    "nn.compile(loss=fn, optimizer='adam', metrics=['accuracy'])\n",
    "nn.summary()\n",
    "\n",
    "\n",
    "# train model or load a trained model\n",
    "TRAIN_MODEL = True\n",
    "\n",
    "if (TRAIN_MODEL): \n",
    "    nn.fit(xn_train, y_train_b, batch_size=128, epochs=500, verbose=1, shuffle=False)\n",
    "    nn.save_weights(\"ugr_nnsmall.h5\")     \n",
    "else:    \n",
    "    nn.load_weights(\"ugr_nnsmall.h5\")\n",
    "        \n",
    "\n",
    "# evaluate model accuracy        \n",
    "score = nn.evaluate(xn_train, y_train_b, verbose=0) #Compute training set accuracy\n",
    "#print('Train loss:', score[0])\n",
    "print('Train accuracy:', score[1])\n",
    "\n",
    "score = nn.evaluate(xn_test, y_test_b, verbose=0) #Compute test set accuracy\n",
    "#print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"good\"></a>\n",
    "### Step 3: Obtain similar samples as explanations for a UGR applicant predicted as \"background\" (Example 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Normalize the data and chose a particular applicant, whose profile is displayed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_train = nn.predict_classes(xn_train) # Use trained neural network to predict train points\n",
    "p_train = p_train.reshape((p_train.shape[0],1))\n",
    "\n",
    "z_train = np.hstack((xn_train, p_train)) # Store (normalized) instances that were predicted as Good\n",
    "z_train_good = z_train[z_train[:,-1]==1, :]\n",
    "\n",
    "\n",
    "zun_train = np.hstack((x_train, p_train)) # Store (unnormalized) instances that were predicted as Good \n",
    "zun_train_good = zun_train[zun_train[:,-1]==1, :]\n",
    "print(z_train_good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now consider applicant 8 whose loan was approved. Note that this applicant was also considered for the contrastive explainer, however, we now justify the approved status in a different manner using prototypical examples, which is arguably a better explanation for a bank employee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 8\n",
    "\n",
    "X = xn_test[idx].reshape((1,) + xn_test[idx].shape)\n",
    "\n",
    "print(\"Chosen Sample:\", idx)\n",
    "print(\"Prediction made by the model:\", class_names[np.argmax(nn.predict_proba(X))])\n",
    "print(\"Prediction probabilities:\", nn.predict_proba(X))\n",
    "print(\"\")\n",
    "\n",
    "# attach the prediction made by the model to X\n",
    "X = np.hstack((X, nn.predict_classes(X).reshape((1,1))))\n",
    "\n",
    "Xun = x_test[idx].reshape((1,) + x_test[idx].shape) \n",
    "dfx = pd.DataFrame.from_records(Xun.astype('double')) # Create dataframe with original feature values\n",
    "dfx[23] = class_names[int(X[0, -1])]\n",
    "dfx.columns = df.columns\n",
    "dfx.transpose()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Find similar applicants predicted as \"good\" using the protodash explainer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = ProtodashExplainer()\n",
    "(W, S, setValues) = explainer.explain(X, z_train_good, m=5) # Return weights W, Prototypes S and objective function values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Display similar applicant user profiles and the extent to which they are similar to the chosen applicant as indicated by the last row in the table below labelled as \"Weight\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = pd.DataFrame.from_records(zun_train_good[S, 0:-1].astype('double'))\n",
    "RP=[]\n",
    "for i in range(S.shape[0]):\n",
    "    RP.append(class_names[int(z_train_good[S[i], -1])]) # Append class names\n",
    "dfs[23] = RP\n",
    "dfs.columns = df.columns  \n",
    "dfs[\"Weight\"] = np.around(W, 5)/np.sum(np.around(W, 5)) # Calculate normalized importance weights\n",
    "dfs.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Compute how similar a feature of a prototypical user is to the chosen applicant.\n",
    "The more similar the feature of prototypical user is to the applicant, the closer its weight is to 1. We can see below that several features for prototypes are quite similar to the chosen applicant. A human friendly explanation is provided thereafter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = z_train_good[S, 0:-1] # Store chosen prototypes\n",
    "eps = 1e-10 # Small constant defined to eliminate divide-by-zero errors\n",
    "fwt = np.zeros(z.shape)\n",
    "for i in range (z.shape[0]):\n",
    "    for j in range(z.shape[1]):\n",
    "        fwt[i, j] = np.exp(-1 * abs(X[0, j] - z[i,j])/(np.std(z[:, j])+eps)) # Compute feature similarity in [0,1]\n",
    "                \n",
    "# move wts to a dataframe to display\n",
    "dfw = pd.DataFrame.from_records(np.around(fwt.astype('double'), 2))\n",
    "dfw.columns = df.columns[:-1]\n",
    "dfw.transpose()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "The above table depicts the five closest user profiles to the chosen applicant. Based on importance weight outputted by the method, we see that the prototype under column zero is the most representative user profile by far. This is (intuitively) confirmed from the feature similarity table above where more than 50% of the features (12 out of 23) of this prototype are identical to that of the chosen user whose prediction we want to explain. Also, the bank employee looking at the prototypical users and their features surmises that the approved applicant belongs to a group of approved users that have practically no debt (NetFractionInstallBurden). This justification gives the employee more confidence in approving the users application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"bad\"></a>\n",
    "### Example 2. Obtaining similar samples as explanations for a connection predicted as \"blacklist\". \n",
    "We now consider a user 1272 whose loan was denied. We obtained a contrastive explanation for this user before. Similar to user 8, we now obtain exemplar based explanations for this user to help the bank employee understand the reasons for the rejection. Steps similar to example 1 are followed in this case too, where we first process the data, obtain prototypes and their importance weights, and finally showcase how similar the features are of these prototypes to the user we want to explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Normalize the data and chose a particular applicant, whose profile is displayed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_train_bad = z_train[z_train[:,-1]==0, :]\n",
    "zun_train_bad = zun_train[zun_train[:,-1]==0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 272 #another user to try 2385\n",
    "\n",
    "X = xn_test[idx].reshape((1,) + xn_test[idx].shape)\n",
    "print(\"Chosen Sample:\", idx)\n",
    "print(\"Prediction made by the model:\", class_names[np.argmax(nn.predict_proba(X))])\n",
    "print(\"Prediction probabilities:\", nn.predict_proba(X))\n",
    "print(\"\")\n",
    "\n",
    "X = np.hstack((X, nn.predict_classes(X).reshape((1,1))))\n",
    "\n",
    "# move samples to a dataframe to display\n",
    "Xun = x_test[idx].reshape((1,) + x_test[idx].shape)\n",
    "dfx = pd.DataFrame.from_records(Xun.astype('double'))\n",
    "dfx[23] = class_names[int(X[0, -1])]\n",
    "dfx.columns = df.columns\n",
    "dfx.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Find similar applicants predicted as \"bad\" using the protodash explainer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(W, S, setValues) = explainer.explain(X, z_train_bad, m=5) # Return weights W, Prototypes S and objective function values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Display similar applicant user profiles and the extent to which they are similar to the chosen applicant as indicated by the last row in the table below labelled as \"Weight\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move samples to a dataframe to display\n",
    "dfs = pd.DataFrame.from_records(zun_train_bad[S, 0:-1].astype('double'))\n",
    "RP=[]\n",
    "for i in range(S.shape[0]):\n",
    "    RP.append(class_names[int(z_train_bad[S[i], -1])]) # Append class names\n",
    "dfs[23] = RP\n",
    "dfs.columns = df.columns  \n",
    "dfs[\"Weight\"] = np.around(W, 5)/np.sum(np.around(W, 5)) # Compute normalized importance weights for prototypes\n",
    "dfs.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Compute how similar a feature of a prototypical user is to the chosen applicant.\n",
    "The more similar the feature of prototypical user is to the applicant, the closer its weight is to 1. We can see below that several features for prototypes are quite similar to the chosen applicant. Following this table we provide human friendly explanation based on this table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = z_train_bad[S, 0:-1] # Store the prototypes\n",
    "eps = 1e-10 # Small constant to guard against divide by zero errors\n",
    "fwt = np.zeros(z.shape)\n",
    "for i in range (z.shape[0]): # Compute feature similarity for each prototype\n",
    "    for j in range(z.shape[1]):\n",
    "        fwt[i, j] = np.exp(-1 * abs(X[0, j] - z[i,j])/(np.std(z[:, j])+eps))\n",
    "                \n",
    "# move wts to a dataframe to display\n",
    "dfw = pd.DataFrame.from_records(np.around(fwt.astype('double'), 2))\n",
    "dfw.columns = df.columns[:-1]\n",
    "dfw.transpose()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "Here again, the above table depicts the five closest user profiles to the chosen applicant. Based on importance weight outputted by the method we see that the prototype under column zero is the most representative user profile by far. This is (intuitively) confirmed from the feature similarity table above where 10 features out of 23 of this prototype are highly similar (>0.9) to that of the user we want to explain. Also the bank employee can see that the applicant belongs to a group of rejected applicants with similar deliquency behavior. Realizing that the user also poses similar risk as these other applicants whose loan was rejected, the employee takes the more conservative decision of rejecting the users application as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"contrastive\"></a>\n",
    "## 4. Customer: Contrastive explanations for UGR Use Case\n",
    "\n",
    "We now demonstrate how to compute contrastive explanations using AIX360 and how such explanations can help home owners understand the decisions made by AI models that approve or reject their HELOC applications. \n",
    "\n",
    "Typically, home owners would like to understand why they do not qualify for a line of credit and if so what changes in their application would qualify them. On the other hand, if they qualified, they might want to know what factors led to the approval of their application. \n",
    "\n",
    "In this context, contrastive explanations provide information to applicants about what minimal changes to their profile would have changed the decision of the AI model from reject to accept or vice-versa (_pertinent negatives_). For example, increasing the number of satisfactory trades to a certain value may have led to the acceptance of the application everything else being the same. \n",
    "\n",
    "The method presented here also highlights a minimal set of features and their values that would still maintain the original decision (_pertinent positives_). For example, for an applicant whose HELOC application was approved, the \n",
    "explanation may say that even if the number of satisfactory trades was reduced to a lower number, the loan would have still gotten through.\n",
    "\n",
    "Additionally, organizations (Banks, financial institutions, etc.) would like to understand trends in the behavior of their AI models in approving loan applications, which could be done by studying contrastive explanations for individuals whose loans were either accepted or rejected. Looking at the aggregate statistics of pertinent positives for approved applicants the organization can get insight into what minimal set of features and their values play an important role in acceptances. While studying the aggregate statistics of pertinent negatives the organization can get insight into features that could change the status of rejected applicants and potentially uncover ways that an applicant may game the system by changing potentially non-important features that could alter the models outcome. \n",
    "\n",
    "The contrastive explanations in AIX360 are implemented using the algorithm developed in the following work:\n",
    "###### [Explanations based on the Missing: Towards Contrastive Explanations with Pertinent Negatives](https://arxiv.org/abs/1802.07623)\n",
    "\n",
    "We now provide a brief overview of the method. As mentioned above the algorithm outputs a contrastive explanation which consists of two parts: a) pertinent negatives (PNs) and b) pertinent positives (PPs). PNs identify a minimal set of features which if altered would change the classification of the original input. For example, in the loan case if a person's credit score is increased their loan application status may change from reject to accept. The manner in which the method accomplishes this is by optimizing a change in the prediction probability loss while enforcing an elastic norm constraint that results in minimal change of features and their values. Optionally, an auto-encoder may also be used to force these minimal changes to produce realistic PNs. PPs on the other hand identify a minimal set of features and their values that are sufficient to yield the original input's classification. For example, an individual's loan may still be accepted if the salary was 50K as opposed to 100K. Here again we have an elastic norm term so that the amount of information needed is minimal, however, the first loss term in this case tries to make the original input's class to be the winning class. For a more in-depth discussion, please refer to the above work.\n",
    "\n",
    "\n",
    "The three main steps to obtain a contrastive explanation are shown below. The first two steps are more about processing the data and building an AI model while the third step computes the actual explanation. \n",
    "\n",
    " [Step 1. Process and Normalize HELOC dataset for training](#c1)<br>\n",
    " [Step 2. Define and train a NN classifier](#c2)<br>\n",
    " [Step 3. Compute contrastive explanations for a few applicants](#c3)<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load  dataset and show sample applicants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "heloc = UGRDataset()\n",
    "df = heloc.dataframe()\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 24)\n",
    "pd.set_option('display.width', 1000)\n",
    "print(\"Size of UGR dataset:\", df.shape)\n",
    "print(\"Number of \\\"Normal\\\" connections:\", np.sum(df['Label']=='normal'))\n",
    "print(\"Number of \\\"Abnormal\\\" connections:\", np.sum(df['Label']=='abnormal'))\n",
    "print(\"Sample Connections:\")\n",
    "df.head(10).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot (example) distributions for two features\n",
    "print(\"Distribution of FlowDuration and Bytes columns:\")\n",
    "hist = df.hist(column=['FlowDuration', 'Bytes'], bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"c1\"></a>\n",
    "### Step 1. Process and Normalize  dataset for training\n",
    "\n",
    "We will first process the HELOC dataset before using it to train an NN model that can predict the\n",
    "target variable RiskPerformance. The HELOC dataset is a tabular dataset with numerical values. However, some of the values are negative and need to be filtered. The processed data is stored in the file heloc.npz for easy access. The dataset is also normalized for training.\n",
    "\n",
    "The data processing and model building is very similar to the Loan Officer persona above, where ProtoDash was the method of choice. We repeat these steps here so that both the use cases can be run independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Process the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data and split dataset into train/test\n",
    "PROCESS_DATA = False\n",
    "\n",
    "if (PROCESS_DATA): \n",
    "    (Data, x_train, x_test, y_train_b, y_test_b) = heloc.split()\n",
    "    np.savez('heloc.npz', Data=Data, x_train=x_train, x_test=x_test, y_train_b=y_train_b, y_test_b=y_test_b)\n",
    "else:\n",
    "    heloc = np.load('heloc.npz', allow_pickle = True)\n",
    "    Data = heloc['Data']\n",
    "    x_train = heloc['x_train']\n",
    "    x_test  = heloc['x_test']\n",
    "    y_train_b = heloc['y_train_b']\n",
    "    y_test_b  = heloc['y_test_b']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### b. Normalize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.vstack((x_train, x_test))\n",
    "Zmax = np.max(Z, axis=0)\n",
    "Zmin = np.min(Z, axis=0)\n",
    "\n",
    "#normalize an array of samples to range [-0.5, 0.5]\n",
    "def normalize(V):\n",
    "    VN = (V - Zmin)/(Zmax - Zmin)\n",
    "    VN = VN - 0.5\n",
    "    return(VN)\n",
    "    \n",
    "# rescale a sample to recover original values for normalized values. \n",
    "def rescale(X):\n",
    "    return(np.multiply ( X + 0.5, (Zmax - Zmin) ) + Zmin)\n",
    "\n",
    "N = normalize(Z)\n",
    "xn_train = N[0:x_train.shape[0], :]\n",
    "xn_test  = N[x_train.shape[0]:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"c2\"></a>\n",
    "### Step 2. Define and train a NN classifier\n",
    "\n",
    "Let us now build a loan approval model based on the HELOC dataset.\n",
    "\n",
    "#### a. Define NN architecture\n",
    "We now define the architecture of a 2-layer neural network classifier whose predictions we will try to interpret. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn with no softmax\n",
    "def nn_small():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=11, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(2, kernel_initializer='normal'))    \n",
    "    return model    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Train the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for repeatability\n",
    "np.random.seed(1) \n",
    "tf.set_random_seed(2) \n",
    "\n",
    "class_names = ['background', 'blacklist']\n",
    "\n",
    "# loss function\n",
    "def fn(correct, predicted):\n",
    "    return tf.nn.softmax_cross_entropy_with_logits(labels=correct, logits=predicted)\n",
    "\n",
    "# compile and print model summary\n",
    "nn = nn_small()\n",
    "nn.compile(loss=fn, optimizer='adam', metrics=['accuracy'])\n",
    "nn.summary()\n",
    "\n",
    "\n",
    "# train model or load a trained model\n",
    "TRAIN_MODEL = True\n",
    "\n",
    "if (TRAIN_MODEL):             \n",
    "    nn.fit(xn_train, y_train_b, batch_size=128, epochs=500, verbose=1, shuffle=False)\n",
    "    nn.save_weights(\"heloc_nnsmall.h5\")     \n",
    "else:    \n",
    "    nn.load_weights(\"heloc_nnsmall.h5\")\n",
    "        \n",
    "\n",
    "# evaluate model accuracy        \n",
    "score = nn.evaluate(xn_train, y_train_b, verbose=0) #Compute training set accuracy\n",
    "#print('Train loss:', score[0])\n",
    "print('Train accuracy:', score[1])\n",
    "\n",
    "score = nn.evaluate(xn_test, y_test_b, verbose=0) #Compute test set accuracy\n",
    "#print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"c3\"></a>\n",
    "### Step 3. Compute contrastive explanations for a few applicants\n",
    "\n",
    "Given the trained NN model to decide on loan approvals, let us first examine an applicant whose application was denied and what (minimal) changes to his/her application would lead to approval (i.e. finding pertinent negatives). We will then look at another applicant whose loan was approved and ascertain features that would minimally suffice in him/her still getting a positive outcome (i.e. finding pertinent positives).\n",
    "\n",
    "#### a. Compute Pertinent Negatives (PN): \n",
    "\n",
    "In order to compute pertinent negatives, the CEM explainer computes a user profile that is close to the original applicant but for whom the decision of HELOC application is different. The explainer alters a minimal set of features by a minimal (positive) amount. This will help the user whose loan application was initially rejected say, to ascertain how to get it accepted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some interesting user samples to try: 2344 449 1168 1272\n",
    "idx = 272\n",
    "\n",
    "X = xn_test[idx].reshape((1,) + xn_test[idx].shape)\n",
    "print(\"Computing PN for Sample:\", idx)\n",
    "print(\"Prediction made by the model:\", nn.predict_proba(X))\n",
    "print(\"Prediction probabilities:\", class_names[np.argmax(nn.predict_proba(X))])\n",
    "print(\"\")\n",
    "\n",
    "mymodel = KerasClassifier(nn)\n",
    "explainer = CEMExplainer(mymodel)\n",
    "\n",
    "arg_mode = 'PN' # Find pertinent negatives\n",
    "arg_max_iter = 1000 # Maximum number of iterations to search for the optimal PN for given parameter settings\n",
    "arg_init_const = 10.0 # Initial coefficient value for main loss term that encourages class change\n",
    "arg_b = 9 # No. of updates to the coefficient of the main loss term\n",
    "arg_kappa = 0.1 # Minimum confidence gap between the PNs (changed) class probability and original class' probability\n",
    "arg_beta = 1e-1 # Controls sparsity of the solution (L1 loss)\n",
    "arg_gamma = 100 # Controls how much to adhere to a (optionally trained) auto-encoder\n",
    "my_AE_model = None # Pointer to an auto-encoder\n",
    "\n",
    "# Find PN for applicant 1272\n",
    "(adv_pn, delta_pn, info_pn) = explainer.explain_instance(X, arg_mode, my_AE_model, arg_kappa, arg_b,\n",
    "                                                         arg_max_iter, arg_init_const, arg_beta, arg_gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by examining one particular loan application that was denied for applicant 1272. We showcase below how the decision could have been different through minimal changes to the profile conveyed by the pertinent negative. We also indicate the importance of different features to produce the change in the application status. The column delta in the table below indicates the necessary deviations for each of the features to produce this change. A human friendly explanation is then provided based on these deviations following the feature importance plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xpn = adv_pn\n",
    "classes = [ class_names[np.argmax(nn.predict_proba(X))], class_names[np.argmax(nn.predict_proba(Xpn))], 'NIL' ]\n",
    "\n",
    "print(\"Sample:\", idx)\n",
    "print(\"prediction(X)\", nn.predict_proba(X), class_names[np.argmax(nn.predict_proba(X))])\n",
    "print(\"prediction(Xpn)\", nn.predict_proba(Xpn), class_names[np.argmax(nn.predict_proba(Xpn))] )\n",
    "\n",
    "\n",
    "X_re = rescale(X) # Convert values back to original scale from normalized\n",
    "Xpn_re = rescale(Xpn)\n",
    "Xpn_re = np.around(Xpn_re.astype(np.double), 2)\n",
    "\n",
    "delta_re = Xpn_re - X_re\n",
    "delta_re = np.around(delta_re.astype(np.double), 2)\n",
    "delta_re[np.absolute(delta_re) < 1e-4] = 0\n",
    "\n",
    "X3 = np.vstack((X_re, Xpn_re, delta_re))\n",
    "\n",
    "dfre = pd.DataFrame.from_records(X3) # Create dataframe to display original point, PN and difference (delta)\n",
    "dfre[23] = classes\n",
    "\n",
    "dfre.columns = df.columns\n",
    "dfre.rename(index={0:'X',1:'X_PN', 2:'(X_PN - X)'}, inplace=True)\n",
    "dfret = dfre.transpose()\n",
    "\n",
    "\n",
    "def highlight_ce(s, col, ncols):\n",
    "    if (type(s[col]) != str):\n",
    "        if (s[col] > 0):\n",
    "            return(['background-color: yellow']*ncols)    \n",
    "    return(['background-color: white']*ncols)\n",
    "\n",
    "dfret.style.apply(highlight_ce, col='(X_PN - X)', ncols=3, axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us compute the importance of different PN features that would be instrumental in 1272 receiving a favorable outcome and display below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "fi = abs((X-Xpn).astype('double'))/np.std(xn_train.astype('double'), axis=0) # Compute PN feature importance\n",
    "objects = df.columns[-2::-1]\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = fi[0, -1::-1]\n",
    "\n",
    "plt.barh(y_pos, performance, align='center', alpha=0.5) # bar chart\n",
    "plt.yticks(y_pos, objects) # Display features on y-axis\n",
    "plt.xlabel('weight') # x-label\n",
    "plt.title('PN (feature importance)') # Heading\n",
    "\n",
    "plt.show() # Display PN feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation:  \n",
    "We observe that the applicant 1272's loan application would have been accepted if the consolidated risk marker score  (i.e. ExternalRiskEstimate) increased from 65 to 76, the loan application was on file (i.e. AverageMlnFile) for about 65 months and if the number of satisfactory trades (i.e. NumSatisfactoryTrades) increased to little over 20.\n",
    "\n",
    "_The above changes to the three suggested factors are also intuitively consistent in improving the chances of acceptance of an application, since all three are monotonic with probability of acceptance (refer HELOC description table). \n",
    "However, one must realize that the above explanation is for the particular applicant based on what the model would do and does not necessarily have to agree with their intuitive meaning. In fact, if the explanation is deemed unacceptable then its an indication that perhaps the model should be debugged/updated_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Pertinent Positives (PP):\n",
    "In order to compute pertinent positives, the CEM explainer identifies a minimal set of features along with their values (as close to 0) that would still maintain the predicted loan application status of the applicant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some interesting user samples to try: 8 9 11\n",
    "idx = 8\n",
    "\n",
    "X = xn_test[idx].reshape((1,) + xn_test[idx].shape)\n",
    "print(\"Computing PP for Sample:\", idx)\n",
    "print(\"Prediction made by the model:\", class_names[np.argmax(nn.predict_proba(X))])\n",
    "print(\"Prediction probabilities:\", nn.predict_proba(X))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "mymodel = KerasClassifier(nn)\n",
    "explainer = CEMExplainer(mymodel)\n",
    "\n",
    "arg_mode = 'PP' # Find pertinent positives\n",
    "arg_max_iter = 1000 # Maximum number of iterations to search for the optimal PN for given parameter settings\n",
    "arg_init_const = 10.0 # Initial coefficient value for main loss term that encourages class change\n",
    "arg_b = 9 # No. of updates to the coefficient of the main loss term\n",
    "arg_kappa = 0.1 # Minimum confidence gap between the PNs (changed) class probability and original class' probability\n",
    "arg_beta = 1e-1 # Controls sparsity of the solution (L1 loss)\n",
    "arg_gamma = 100 # Controls how much to adhere to a (optionally trained) auto-encoder\n",
    "my_AE_model = None # Pointer to an auto-encoder\n",
    "\n",
    "(adv_pp, delta_pp, info_pp) = explainer.explain_instance(X, arg_mode, my_AE_model, arg_kappa, arg_b,\n",
    "                                                         arg_max_iter, arg_init_const, arg_beta, arg_gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the pertinent positives, we look at a different applicant 8 whose loan application was approved. We want to ascertain here what minimal values for this profile would still have lead to acceptance. Below, we showcase the pertinent positive as well as the important features in maintaining the approved status. The 0s in the PP column indicate that those features were not important. The 0s in the PP column indicate that those features were not important. Here too, we provide a human friendly explanation following the feature importance plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xpp = delta_pp\n",
    "classes = [ class_names[np.argmax(nn.predict_proba(X))], class_names[np.argmax(nn.predict_proba(Xpp))]]\n",
    "\n",
    "print(\"PP for Sample:\", idx)\n",
    "print(\"Prediction(Xpp) :\", class_names[np.argmax(nn.predict_proba(Xpp))])\n",
    "print(\"Prediction probabilities for Xpp:\", nn.predict_proba(Xpp))\n",
    "print(\"\")\n",
    "\n",
    "X_re = rescale(X) # Convert values back to original scale from normalized\n",
    "adv_pp_re = rescale(adv_pp)\n",
    "Xpp_re = X_re - adv_pp_re\n",
    "Xpp_re = np.around(Xpp_re.astype(np.double), 2)\n",
    "Xpp_re[Xpp_re < 1e-4] = 0\n",
    "\n",
    "X2 = np.vstack((X_re, Xpp_re))\n",
    "\n",
    "dfpp = pd.DataFrame.from_records(X2.astype('double')) # Showcase a dataframe for the original point and PP\n",
    "dfpp[23] = classes\n",
    "dfpp.columns = df.columns\n",
    "dfpp.rename(index={0:'X',1:'X_PP'}, inplace=True)\n",
    "dfppt = dfpp.transpose()\n",
    "\n",
    "dfppt.style.apply(highlight_ce, col='X_PP', ncols=2, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "fi = abs(Xpp_re.astype('double'))/np.std(x_train.astype('double'), axis=0) # Compute PP feature importance\n",
    "    \n",
    "objects = df.columns[-2::-1]\n",
    "y_pos = np.arange(len(objects)) # Get input feature names\n",
    "performance = fi[0, -1::-1]\n",
    "\n",
    "plt.barh(y_pos, performance, align='center', alpha=0.5) # Bar chart\n",
    "plt.yticks(y_pos, objects) # Plot feature names on y-axis\n",
    "plt.xlabel('weight') #x-label\n",
    "plt.title('PP (feature importance)') # Figure heading\n",
    "\n",
    "plt.show()    # Display the feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation:  \n",
    "We observe that the applicant 8's loan application would still have been accepted even if the consolidated risk marker score (i.e. ExternalRiskEstimate) reduced from 82 to around 40, application was on file (i.e. AverageMlnFile) for close to 70 months and number of satisfactory trades (i.e. NumSatisfactoryTrades) reduced from 22 to almost single digits.\n",
    "\n",
    "_Note that explanations may change a bit based on equivalent values in a local minima._"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
